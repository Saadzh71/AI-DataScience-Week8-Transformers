{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed Tutorial: Transformer from Scratch for Tabular Data\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Understanding Transformers\n",
        "\n",
        "Before we dive into the code, it's important to understand what a **Transformer** is and why it works so well for various tasks, including tabular data.\n",
        "\n",
        "The **Transformer** architecture was introduced in the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) and revolutionized tasks like NLP, but can be applied to other types of data too.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "- **Self-Attention**: A mechanism that allows the model to weigh the importance of different input tokens/positions for each input token.\n",
        "- **Multi-Head Attention**: Allows the model to focus on different parts of the input simultaneously.\n",
        "- **Feed Forward Network (FFN)**: After attention, the transformed inputs pass through fully connected layers to learn further representations.\n",
        "- **Layer Normalization**: Normalizes the input to each layer for better training stability.\n",
        "- **Residual Connections**: Adds input back to the output of a layer, preventing the loss of information during transformations.\n",
        "\n",
        "### Why Use Transformers for Tabular Data?\n",
        "- Tabular data often contains both categorical and numerical features.\n",
        "- A transformerâ€™s ability to learn **relationships** between features can be helpful for structured data.\n",
        "- It allows each feature to \"attend\" to other features.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Implementing Multi-Head Self-Attention\n",
        "\n",
        "### What is Self-Attention?\n",
        "\n",
        "Self-attention allows each feature (or token in NLP) to focus on other features while processing, providing a context. For example, in a tabular dataset, if we are processing the \"age\" feature, self-attention helps the model figure out the influence of other features (like \"blood pressure\" or \"cholesterol\") on the target.\n",
        "\n",
        "### Multi-Head Self-Attention:\n",
        "\n",
        "Instead of one attention mechanism, **multi-head attention** performs attention in parallel across multiple heads, learning different aspects of relationships in data.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        # Ensure that embedding dimension is divisible by number of heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(f\"Embedding dimension {embed_dim} should be divisible by number of heads {num_heads}.\")\n",
        "        \n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "    \n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        # Split into multiple heads\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        \n",
        "        # Linear projections for query, key, and value\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        \n",
        "        # Split and perform attention\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "        attention_output, _ = self.attention(query, key, value)\n",
        "        \n",
        "        # Concatenate and project output\n",
        "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.embed_dim))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output\n",
        "```\n",
        "###Explanation:\n",
        "* Query, Key, Value: The core components of attention. Query is what we're currently focusing on, Key is a reference to other parts of the data, and Value is the information associated with those keys.\n",
        "* Matmul and Softmax: We compute a similarity score between query and key using matrix multiplication, then normalize it with softmax.\n",
        "* Multi-Head: Multiple attention mechanisms are applied in parallel, allowing the model to focus on different aspects of the data.\n",
        "* Combine Heads: After processing with multiple heads, we combine the results into one.\n"
      ],
      "metadata": {
        "id": "SKdJTlE4Wt9F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xvxxevm85SHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Building the Transformer Block\n",
        "\n",
        "The **Transformer Block** combines multi-head attention with a feed-forward network. After each step, we normalize the output and add **dropout** for regularization.\n",
        "\n",
        "```python\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "```\n",
        "### Explanation:\n",
        "* Layer Normalization: Normalizes inputs, which helps in stable and faster training.\n",
        "* Feed-Forward Network (FFN): Fully connected layers (Dense) that learn more abstract features from the output of the attention mechanism.\n",
        "* Residual Connection: By adding the input back into the output, we prevent information from being lost."
      ],
      "metadata": {
        "id": "4j-knMBDXpY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "OyaP8YkWa2k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F--eHe5U5QhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Preparation\n",
        "\n",
        "We will now prepare a **tabular dataset**. In this example, we use the heart disease dataset from TensorFlow, focusing on numerical columns.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Example dataset (replace with your actual dataset)\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Define numerical and categorical columns\n",
        "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "target_col = 'target'\n",
        "\n",
        "# Process the features and labels\n",
        "X = df[numerical_cols]\n",
        "y = df[target_col]\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "###Explanation:\n",
        "* Scaling: We normalize the data to have mean 0 and standard deviation 1 to ensure that features are on the same scale.\n",
        "* Encoding: The target is converted to numerical values.\n",
        "* Splitting: We divide the dataset into training and validation sets, with 80% training and 20% validation."
      ],
      "metadata": {
        "id": "-JW-ButgZp1r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gbmn0ptO5QHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Building the Transformer Model\n",
        "\n",
        "The model integrates the **Transformer Block** with a feed-forward classifier for tabular data.\n",
        "\n",
        "```python\n",
        "def build_transformer_model(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    \n",
        "    # Add Transformer block\n",
        "    transformer_block = TransformerBlock(embed_dim=32, num_heads=4, ff_dim=64)\n",
        "    x = transformer_block(inputs, training=True)\n",
        "    \n",
        "    # Global pooling and dense layers\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    x = layers.Dense(16, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Get the input shape (number of features) and number of classes\n",
        "input_shape = (X_train.shape[1], 1)  # Add a dummy dimension for time step\n",
        "num_classes = len(set(y_train))\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_transformer_model(input_shape=input_shape, num_classes=num_classes)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "###Explanation:\n",
        "* Global Pooling: Reduces the output size by averaging across all tokens, which is essential for reducing dimensionality in tabular data.\n",
        "* Dense Layers: Adds fully connected layers to output the final classification result."
      ],
      "metadata": {
        "id": "8WR5ExxhZpyU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-B086jBp5PmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training and Plotting the Loss\n",
        "\n",
        "Finally, we train the model and visualize how the loss evolves over time.\n",
        "\n",
        "```python\n",
        "# Reshape X_train and X_val to match input shape for the transformer\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
        "\n",
        "# Plot the training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "###Explanation:\n",
        "* Reshape: We reshape the data to match the expected input for the transformer.\n",
        "* Training: We train the model and track the loss for both training and validation data.\n",
        "* Plotting: Loss curves help monitor the modelâ€™s performance and spot any overfitting."
      ],
      "metadata": {
        "id": "vSdJql0YbcIr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "di_Q3UjPbuww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}