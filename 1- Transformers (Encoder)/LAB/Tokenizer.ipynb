{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e91ead9",
      "metadata": {
        "id": "4e91ead9"
      },
      "source": [
        "# Tokenization Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "394b1e29",
      "metadata": {
        "id": "394b1e29"
      },
      "source": [
        "\n",
        "Tokenization is the process of breaking text into smaller units such as characters, words, or sentences.\n",
        "In this tutorial, we will cover the following types of tokenization:\n",
        "\n",
        "1. Character Tokenization\n",
        "2. Word Tokenization\n",
        "3. Sentence Tokenization\n",
        "4. Byte Pair Encoding (BPE)\n",
        "\n",
        "We will use Python libraries like `nltk` and `tokenizers` to achieve these tokenizations. Let's start!\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a11f74",
      "metadata": {
        "id": "87a11f74"
      },
      "source": [
        "## 1. Character Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285fef82",
      "metadata": {
        "id": "285fef82"
      },
      "source": [
        "\n",
        "Character tokenization is the process of splitting text into individual characters. This method treats each character as a token.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4df9a18c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4df9a18c",
        "outputId": "041a81fc-4a67-4aff-9d9a-20c1bed8986a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T', '5', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'g', 'r', 'e', 'a', 't', 'e', 's', 't', ' ', 'd', 'a', 't', 'a', ' ', 's', 'c', 'i', 'e', 'n', 'c', 'e', ' ', 'b', 'o', 'o', 't', '-', 'c', 'a', 'm', 'p', '!']\n"
          ]
        }
      ],
      "source": [
        "# Example of character tokenization\n",
        "\n",
        "text = \"T5 is the greatest data science boot-camp!\"\n",
        "char_tokens = list(text)\n",
        "print(char_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f33a8aa",
      "metadata": {
        "id": "2f33a8aa"
      },
      "source": [
        "## 2. Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45c0956",
      "metadata": {
        "id": "d45c0956"
      },
      "source": [
        "\n",
        "Word tokenization splits text into individual words. Commonly, spaces are used as delimiters to break text into words.\n",
        "We will use the `nltk` library to achieve word tokenization.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7e8e6878",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e8e6878",
        "outputId": "0567a83f-fb34-4ccd-defc-fab7fb5fb1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T5', 'is', 'the', 'greatest', 'data', 'science', 'boot-camp', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example of word tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"T5 is the greatest data science boot-camp!\"\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44c5ce1",
      "metadata": {
        "id": "b44c5ce1"
      },
      "source": [
        "## 3. Sentence Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d74612",
      "metadata": {
        "id": "55d74612"
      },
      "source": [
        "\n",
        "Sentence tokenization breaks a text into individual sentences. The `nltk` library provides a sentence tokenizer that can handle different punctuation marks.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "229552a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229552a5",
        "outputId": "28fc3501-d052-4cbc-cf83-5d0874114d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T5 is the greatest boot-camp!', 'it is for data science !']\n"
          ]
        }
      ],
      "source": [
        "# Example of sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"T5 is the greatest boot-camp! it is for data science !\"\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(sentence_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d69391",
      "metadata": {
        "id": "81d69391"
      },
      "source": [
        "## 4. Byte Pair Encoding (BPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ebe5a5b",
      "metadata": {
        "id": "6ebe5a5b"
      },
      "source": [
        "\n",
        "Byte Pair Encoding (BPE) is a subword tokenization algorithm. It is commonly used in large models like GPT to break down text into smaller subword units.\n",
        "We will use the `tokenizers` library to perform BPE tokenization.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "73642689",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73642689",
        "outputId": "209bb619-10ab-46f4-f383-1113f6979d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T5', 'is', 'fun', '!']\n"
          ]
        }
      ],
      "source": [
        "# Install the tokenizers library first if you don't have it\n",
        "# !pip install tokenizers\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Example of BPE tokenization\n",
        "tokenizer = Tokenizer(BPE())\n",
        "trainer = BpeTrainer(vocab_size=1000)\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "texts = [\"T5 is fun!\", \"Byte Pair Encoding is powerful.\"]\n",
        "tokenizer.train_from_iterator(texts, trainer)\n",
        "\n",
        "output = tokenizer.encode(\"T5 is fun!\")\n",
        "print(output.tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}