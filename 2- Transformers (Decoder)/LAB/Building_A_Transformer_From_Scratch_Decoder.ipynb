{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7770c8d",
   "metadata": {},
   "source": [
    "### 1. Importing TensorFlow and Other Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e6c913",
   "metadata": {
    "id": "U2uu6WfGwQFZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dcd4fc",
   "metadata": {},
   "source": [
    "### 2. Defining Positional Encoding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ddf78b4",
   "metadata": {
    "id": "oKkxTLjpwS4t"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :], d_model)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903aef5",
   "metadata": {},
   "source": [
    "- **Explanation**: This class implements positional encoding, which is essential in Transformer models to provide a sense of order to the sequence of inputs. It calculates and applies sine and cosine transformations to create the encoding for each position in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf84334",
   "metadata": {},
   "source": [
    "### 3. Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca92485",
   "metadata": {
    "id": "j-oD3XMJwUUJ"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178da757",
   "metadata": {},
   "source": [
    "- **Explanation**: This function computes scaled dot-product attention, which forms the core of attention mechanisms in transformers. It takes the query, key, and value matrices, scales the dot product of the query and key, applies a mask (if provided), and finally computes the attention-weighted output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788a0b8",
   "metadata": {},
   "source": [
    "### 4. Multi-Head Attention (Truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1035be96",
   "metadata": {
    "id": "AkLTWDc0wV3l"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "\n",
    "        self.dense = layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839689e",
   "metadata": {},
   "source": [
    "- **Explanation**: The implementation for the `MultiHeadAttention` class is truncated, but it follows the transformer architecture where multiple attention heads are used to attend to different parts of the input sequence simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a8c42",
   "metadata": {},
   "source": [
    "### 5. Point-Wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113ef955",
   "metadata": {
    "id": "ibyPM2VLwXf_"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return keras.Sequential([\n",
    "        layers.Dense(dff, activation='relu'),\n",
    "        layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bcaae",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "* This function creates a point-wise feed forward network, which is a key component in Transformer models.\n",
    "* It uses two fully connected dense layers.\n",
    "* The first layer has a hidden dimension of `dff` and applies the `ReLU` activation function to introduce non-linearity.\n",
    "* The second layer projects the output back to the original dimensionality `d_model`, without any activation, making it a linear transformation.\n",
    "* This kind of network is applied independently to each position in the sequence, which makes it \"point-wise.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38a6dd",
   "metadata": {},
   "source": [
    "### 6. Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84013495",
   "metadata": {
    "id": "vt6mA-YGwZ2Y"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        attn1 = self.mha1(x, x, x, mask)\n",
    "        attn1 = self.dropout1(attn1, training=True)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=True)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc59329",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "    - **Initialization (`__init__`)**:\n",
    "        - The `DecoderLayer` is a core building block of the Transformer decoder. It consists of two main components:\n",
    "            1. Multi-Head Attention (`mha1`): This layer performs self-attention on the input sequence `x`, allowing the decoder to weigh the importance of different tokens in the sequence.\n",
    "            2. Feed-Forward Network (`ffn`): After attention is applied, the data passes through a point-wise feed-forward network that consists of two dense layers.\n",
    "        - Layer Normalization is applied twice—before and after the feed-forward network—helping stabilize the network's training.\n",
    "        - Dropout is used to prevent overfitting by randomly deactivating some neurons during training.\n",
    "\n",
    "    - **Forward Pass (`call`)**:\n",
    "        - **Step 1**: Multi-head attention is applied to the input `x`, and the result is stored in `attn1`. Dropout is applied to regularize the attention output.\n",
    "        - **Step 2**: The result from the attention layer (`attn1`) is added back to the input `x` (residual connection) and normalized using `layernorm1`. This produces `out1`.\n",
    "        - **Step 3**: The `out1` result is passed through the feed-forward network (`ffn_output`), and dropout is applied again for regularization.\n",
    "        - **Step 4**: The output of the feed-forward network is added back to `out1` (residual connection) and normalized again via `layernorm2`, producing the final output (`out2`).\n",
    "    \n",
    "    - **Residual Connections**: The addition of the input (`x`) to the output of the attention and feed-forward networks is a crucial part of the Transformer architecture. It ensures that the model retains information from earlier layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbadb435",
   "metadata": {},
   "source": [
    "### 7. Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbf6629",
   "metadata": {
    "id": "_d0P1rcGwbkH"
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_positional_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_positional_encoding)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        x = self.dropout(x, training=True)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda07be5",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "    - **Initialization (`__init__`)**:\n",
    "        - The `Decoder` class represents the entire decoder module in the Transformer model, consisting of multiple `DecoderLayer` instances stacked together.\n",
    "        - **Attributes**:\n",
    "            - `d_model`: The dimensionality of the embedding space.\n",
    "            - `num_layers`: The number of stacked decoder layers.\n",
    "            - **Embedding Layer**: The target sequence is first passed through an embedding layer that converts tokens to dense vectors of dimension `d_model`.\n",
    "            - **Positional Encoding**: Since the Transformer model does not have recurrence, positional encoding is applied to provide information about the order of the sequence.\n",
    "            - **Decoder Layers**: Multiple `DecoderLayer` instances (as defined earlier) are created and stored in the list `dec_layers`.\n",
    "            - **Dropout**: Dropout is applied after the embedding to regularize the decoder and prevent overfitting.\n",
    "\n",
    "    - **Forward Pass (`call`)**:\n",
    "        - **Input Processing**:\n",
    "            - The input sequence `x` is passed through the embedding layer, which converts tokens into dense vectors.\n",
    "            - The embeddings are then scaled by the square root of the model's dimension (`d_model`) to normalize the variance in the output.\n",
    "            - Positional encoding is applied to give each position in the sequence a unique representation.\n",
    "            - Dropout is applied to the positional encoding output.\n",
    "        \n",
    "        - **Decoder Layers**: The input sequence is passed sequentially through each of the decoder layers, applying self-attention and feed-forward transformations in each layer.\n",
    "\n",
    "    - **Final Output**:\n",
    "        - The final output of the `Decoder` is a tensor with the shape `(batch_size, target_seq_len, d_model)`, where each token in the target sequence is now represented in the `d_model`-dimensional space, ready to be used for further processing (such as generating predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc774b08",
   "metadata": {},
   "source": [
    "### 8. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "966cebcd",
   "metadata": {
    "id": "RACyRd_XweXr"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "class TransformerDecoderModel(keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_positional_encoding, rate=0.1):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, max_positional_encoding, rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, tar):\n",
    "        seq_len = tf.shape(tar)[1]\n",
    "        look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "        dec_output = self.decoder(tar, look_ahead_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81f6c5",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "    - **Look-Ahead Mask (`create_look_ahead_mask`)**:\n",
    "        - The function `create_look_ahead_mask` creates a mask to prevent the decoder from attending to future tokens in the sequence during training.\n",
    "        - The mask is created using TensorFlow's `band_part` function, which generates a lower triangular matrix filled with ones below the diagonal and zeros elsewhere. The diagonal and lower portion are kept, while the upper portion is masked.\n",
    "        - This is essential for autoregressive tasks like language generation, where the model should not see future tokens.\n",
    "\n",
    "    - **TransformerDecoderModel**:\n",
    "        - **Initialization (`__init__`)**:\n",
    "            - This class defines a Transformer decoder model, where the decoder is followed by a final dense layer for generating predictions.\n",
    "            - **Attributes**:\n",
    "                - `decoder`: This is an instance of the `Decoder` class, which consists of multiple layers of attention and feed-forward operations.\n",
    "                - `final_layer`: A dense layer that maps the output of the decoder to the target vocabulary size. This layer is responsible for producing the logits used for token predictions.\n",
    "\n",
    "    - **Forward Pass (`call`)**:\n",
    "        - **Input Processing**:\n",
    "            - The `call` method takes in the target sequence `tar` and computes the sequence length.\n",
    "            - The look-ahead mask is created using the `create_look_ahead_mask` function, which ensures that during training, the model cannot look ahead to future positions in the target sequence.\n",
    "        \n",
    "        - **Decoder**:\n",
    "            - The target sequence, along with the look-ahead mask, is passed to the `Decoder`, which applies the necessary layers (self-attention, feed-forward networks, etc.) to produce the decoder output.\n",
    "        \n",
    "        - **Final Output**:\n",
    "            - The output of the decoder is passed through the `final_layer`, which converts the output into a tensor with dimensions corresponding to the target vocabulary size. This is the prediction layer where the model generates probabilities for each token in the vocabulary.\n",
    "            - The final output is a tensor of shape `(batch_size, target_seq_len, target_vocab_size)` that contains the logits for the vocabulary for each token in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30ed9a",
   "metadata": {},
   "source": [
    "### 9. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "236174da",
   "metadata": {
    "id": "4H3L9PsrwgJR"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "target_vocab_size = 8000\n",
    "max_positional_encoding = 10000\n",
    "\n",
    "decoder_model = TransformerDecoderModel(num_layers, d_model, num_heads, dff, target_vocab_size, max_positional_encoding)\n",
    "\n",
    "decoder_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "929c5db5",
   "metadata": {
    "id": "cwyD-ZZCwiE3"
   },
   "outputs": [],
   "source": [
    "text_file_path = 't8.shakespeare.txt'\n",
    "with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfef5e3f",
   "metadata": {
    "id": "qLSFZW6IxDeM"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True) \n",
    "tokenizer.fit_on_texts([text_data])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences([text_data])[0]\n",
    "\n",
    "sequence_length = 50\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "\n",
    "for i in range(0, len(sequences) - sequence_length):\n",
    "    input_sequences.append(sequences[i:i + sequence_length])\n",
    "    target_sequences.append(sequences[i + 1:i + sequence_length + 1])\n",
    "\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d6c901",
   "metadata": {
    "id": "cMYpBBjwxEk_"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6c7e01",
   "metadata": {
    "id": "Yns-6t4pxHiw"
   },
   "outputs": [],
   "source": [
    "target_vocab_size = vocab_size\n",
    "\n",
    "decoder_model = TransformerDecoderModel(\n",
    "    num_layers=4, d_model=128, num_heads=8, dff=512,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    max_positional_encoding=sequence_length\n",
    ")\n",
    "\n",
    "decoder_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1de993c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B5NV2vjxJql",
    "outputId": "1a911010-65ef-4482-c59d-649908f63b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85283/85283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 5ms/step - loss: 1.0516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d3a10bce2f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1\n",
    "decoder_model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab260150",
   "metadata": {},
   "source": [
    "### 10. Generating Text with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d3ad295",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiCq2yBdxLhT",
    "outputId": "11cc8d47-8e79-4940-b36d-00fd4087c6b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be, that is, boy,\n",
      "    soseless him yet downid like an aim a patient rich\n",
      "  of illinois me; but and swalkl, and \n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, seed_text, num_generate=100):\n",
    "    input_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    input_sequence = np.array(input_sequence).reshape(1, -1)\n",
    "\n",
    "    generated_text = seed_text\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_sequence)\n",
    "        predictions = predictions[:, -1, :]  # Get the last predicted token\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
    "\n",
    "        predicted_char = tokenizer.index_word[predicted_id]\n",
    "        generated_text += predicted_char\n",
    "\n",
    "        input_sequence = np.append(input_sequence[0], predicted_id)[-sequence_length:].reshape(1, -1)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "seed_text = \"To be, or not to be, that is\"\n",
    "generated = generate_text(decoder_model, tokenizer, seed_text)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
