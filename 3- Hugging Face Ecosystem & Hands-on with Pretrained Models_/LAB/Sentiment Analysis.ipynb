{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["AgDsJEm2VhdG","oJ6WQ18_Vj8G"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Sentiment Analysis"],"metadata":{"id":"CAKn6_uphIeg"}},{"cell_type":"markdown","source":["Sentiment analysis helps in identifying opinions, emotions, and attitudes expressed in text data, making it highly valuable for various applications, such as product reviews, social media monitoring, and customer feedback analysis.\n","\n","You will learn:\n","\n","* **Using pre trained sentiment analysis model**\n","* **Fine tune sentiment analysis models**"],"metadata":{"id":"570QR6swh357"}},{"cell_type":"markdown","source":["## Use Pre Trained Model"],"metadata":{"id":"AgDsJEm2VhdG"}},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxPH-_HeVya6","outputId":"4612af42-5605-445d-f7cd-2c6ba0c21253","executionInfo":{"status":"ok","timestamp":1726062086145,"user_tz":-180,"elapsed":19995,"user":{"displayName":"Ali El-Kassas","userId":"11588852990612634810"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"]}]},{"cell_type":"markdown","source":["### Import the necessary libraries"],"metadata":{"id":"8A3WDw1xVBfw"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"IhJ4dZd9VA_U","executionInfo":{"status":"ok","timestamp":1726062117604,"user_tz":-180,"elapsed":31463,"user":{"displayName":"Ali El-Kassas","userId":"11588852990612634810"}}},"outputs":[],"source":["from transformers import pipeline"]},{"cell_type":"markdown","source":["### Create a sentiment analysis pipeline"],"metadata":{"id":"0kVmWPWAVEGm"}},{"cell_type":"markdown","source":["We initialize a sentiment analysis pipeline using the pretrained model distilbert-base-uncased-finetuned-sst-2-english."],"metadata":{"id":"hyT51PDiVHIp"}},{"cell_type":"code","source":["sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNQLMO2aVIWJ","outputId":"d7d23e20-bec3-4a67-aad3-ae8c70c2202f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"]}]},{"cell_type":"markdown","source":["### Provide input text"],"metadata":{"id":"FlUTP6rFVTHF"}},{"cell_type":"markdown","source":["You can provide any text you want to analyze for sentiment."],"metadata":{"id":"JbAKYAPtVVNW"}},{"cell_type":"code","source":["text = \"I love the way transformers are making NLP tasks so much easier!\""],"metadata":{"id":"zNtfCqldVWYJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analyze the sentiment"],"metadata":{"id":"tWIV5YuUVXer"}},{"cell_type":"markdown","source":["Pass the text into the sentiment analyzer pipeline to determine the sentiment.\n"],"metadata":{"id":"ldJDdYunVZKV"}},{"cell_type":"code","source":["result = sentiment_analyzer(text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4F_VEK2VaFL","outputId":"9d776cda-bc56-4ddb-9ff1-ee51d14e21ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.9994403719902039}]\n"]}]},{"cell_type":"markdown","source":["### Explanation of Output"],"metadata":{"id":"sQXrUH9qVbNp"}},{"cell_type":"markdown","source":["*   label: The sentiment of the text (e.g., POSITIVE, NEGATIVE).\n","*   score: A confidence score indicating how certain the model is about its prediction."],"metadata":{"id":"GdbDVvyWVd3d"}},{"cell_type":"markdown","source":["## Fine Tuning"],"metadata":{"id":"oJ6WQ18_Vj8G"}},{"cell_type":"markdown","source":["There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ğŸ¤— Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n","\n","* Fine-tune a pretrained model with ğŸ¤— Transformers Trainer.\n","* Fine-tune a pretrained model in TensorFlow with Keras."],"metadata":{"id":"xXSmKSFqWMed"}},{"cell_type":"markdown","source":["### Install and Import Libraries"],"metadata":{"id":"NTD-aDCvWUrF"}},{"cell_type":"markdown","source":["Train a TensorFlow model with Keras\n","You can also train ğŸ¤— Transformers models in TensorFlow with the Keras API!\n","\n","Loading data for Keras\n","When you want to train a ğŸ¤— Transformers model with the Keras API, you need to convert your dataset to a format that Keras understands. If your dataset is small, you can just convert the whole thing to NumPy arrays and pass it to Keras. Letâ€™s try that first before we do anything more complicated.\n","\n","First, load a dataset. Weâ€™ll use the CoLA dataset from the GLUE benchmark, since itâ€™s a simple binary text classification task, and just take the training split for now."],"metadata":{"id":"WPJLRwP9XLTm"}},{"cell_type":"markdown","source":["### Load the Data"],"metadata":{"id":"Zo1tPl2Oc5ke"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"glue\", \"cola\")\n","dataset = dataset[\"train\"]  # Just take the training split for now"],"metadata":{"id":"G-NyksoWW5DR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Take a Look at the Data"],"metadata":{"id":"q-q9ZMPNgSD-"}},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_V8Wnppc7rG","outputId":"09f80b13-b3ae-4dd5-e26a-f7eff39693f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['sentence', 'label', 'idx'],\n","    num_rows: 8551\n","})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["print(f'First sentence: {dataset[\"sentence\"][0]}')\n","print(f'First label: {dataset[\"label\"][0]}')\n","print(f'First idx: {dataset[\"idx\"][0]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPgpAQocc9mL","outputId":"f4f039d1-2c6b-435a-cea2-2c7a8da27950"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First sentence: Our friends won't buy this analysis, let alone the next one we propose.\n","First label: 1\n","First idx: 0\n"]}]},{"cell_type":"markdown","source":["### Load a Tokenizer"],"metadata":{"id":"8GWOfaIQgYQP"}},{"cell_type":"markdown","source":["Next, load a tokenizer and tokenize the data as NumPy arrays. Note that the labels are already a list of 0 and 1s, so we can just convert that directly to a NumPy array without tokenization!"],"metadata":{"id":"NVXNskXXXe6Q"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","import numpy as np\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n","tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n","# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n","tokenized_data = dict(tokenized_data)\n","\n","labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8apTsJ85XTYk","outputId":"94107ef7-fc04-46d7-b6ce-c13f832ea842"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Finally, load, [compile](https://keras.io/api/models/model_training_apis/#compile-method), and fit the model. Note that Transformers models all have a default task-relevant loss function, so you donâ€™t need to specify one unless you want to:"],"metadata":{"id":"rwx48Q-1X-eS"}},{"cell_type":"markdown","source":["### Load the Model and Fine Tuning it"],"metadata":{"id":"2sofWblaggsQ"}},{"cell_type":"code","source":["from transformers import TFAutoModelForSequenceClassification, pipeline\n","\n","# Load and compile our model\n","model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n","# Lower learning rates are often better for fine-tuning transformers\n","model.compile(optimizer='Adam')  # No loss argument!\n","\n","model.fit(tokenized_data, labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EiaDBhUlX7Os","outputId":"e637e10c-9b06-489d-d699-ffc1978d29f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["268/268 [==============================] - 119s 310ms/step - loss: 0.6487\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf_keras.src.callbacks.History at 0x7a4889ab1390>"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["**Note:**\n","You donâ€™t have to pass a loss argument to your models when you compile() them! Hugging Face models automatically choose a loss that is appropriate for their task and model architecture if this argument is left blank. You can always override this by specifying a loss yourself if you want to!"],"metadata":{"id":"soCCB5CMYF3o"}},{"cell_type":"markdown","source":["This approach works great for smaller datasets, but for larger datasets, you might find it starts to become a problem. Why? Because the tokenized array and labels would have to be fully loaded into memory, and because NumPy doesnâ€™t handle â€œjaggedâ€ arrays, so every tokenized sample would have to be padded to the length of the longest sample in the whole dataset. Thatâ€™s going to make your array even bigger, and all those padding tokens will slow down training too!"],"metadata":{"id":"kLeWasLyYULt"}},{"cell_type":"markdown","source":["### Save the Model"],"metadata":{"id":"93cMVKp2f8oh"}},{"cell_type":"code","source":["model.save_pretrained(\"./cola_finetuned_model\")\n","tokenizer.save_pretrained(\"./cola_finetuned_model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DcbqyLwQYD7D","outputId":"ba5ed20f-1ce2-4ef0-d194-c120f53c947c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('./cola_finetuned_model/tokenizer_config.json',\n"," './cola_finetuned_model/special_tokens_map.json',\n"," './cola_finetuned_model/vocab.txt',\n"," './cola_finetuned_model/added_tokens.json',\n"," './cola_finetuned_model/tokenizer.json')"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["### Load the model"],"metadata":{"id":"CpzysrTsf-3m"}},{"cell_type":"code","source":["classifier = pipeline(\"text-classification\", model=\"/content/cola_finetuned_model\", tokenizer=\"/content/cola_finetuned_model\", return_all_scores=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQbzyqRJeojx","outputId":"93969318-cbee-4b89-c90f-e5d2ba98b893"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at /content/cola_finetuned_model were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/cola_finetuned_model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n","Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["### Inference"],"metadata":{"id":"UYicuV1BgAWg"}},{"cell_type":"code","source":["sentence = \"This is an acceptable sentence.\"\n","result = classifier(sentence)\n","print(f\"Prediction: {result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BxuM0ubeqEn","outputId":"9e4448aa-86df-4689-82f0-70b4e6233b96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction: [{'label': 'LABEL_1', 'score': 0.7155210375785828}]\n"]}]}]}