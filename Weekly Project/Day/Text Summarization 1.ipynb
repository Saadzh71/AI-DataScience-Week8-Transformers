{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00dbdcaa",
   "metadata": {},
   "source": [
    "# Fine-tuning a Model for Summarization Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d11590b",
   "metadata": {},
   "source": [
    "In this task, you will load, preprocess, and fine-tune a T5 model on a dataset of news articles for a summarization task. Follow the steps below carefully.\n",
    "\n",
    "### Model and Dataset Information\n",
    "\n",
    "For this task, you will be working with the following:\n",
    "\n",
    "- **Model Checkpoint**: Use the pre-trained model checkpoint `UBC-NLP/AraT5-base` if you face any problem you can use `google-t5/t5-small` but the first one is the correct one for both the model and tokenizer.\n",
    "- **Dataset**: You will be using the `CUTD/arabic_dialogue_df` dataset. Ensure to load and preprocess the dataset correctly for training and evaluation.\n",
    "\n",
    "**Note:**\n",
    "- Any additional steps or methods you include that improve or enhance the results will be rewarded with bonus points if they are justified.\n",
    "- The steps outlined here are suggestions. You are free to implement alternative methods or approaches to achieve the task, as long as you explain the reasoning and the process at the bottom of the notebook.\n",
    "- You can use either TensorFlow or PyTorch for this task. If you prefer TensorFlow, feel free to use it when working with Hugging Face Transformers.\n",
    "- The number of data samples you choose to work with is flexible. However, if you select a very low number of samples and the training time is too short, this could affect the evaluation of your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bca4c",
   "metadata": {},
   "source": [
    "## Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f385d",
   "metadata": {},
   "source": [
    "Load the dataset and split it into training and test sets. Use 20% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb08e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d518350b",
   "metadata": {},
   "source": [
    "## Step 2: Load the Pretrained Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a333d",
   "metadata": {},
   "source": [
    "Initialize a tokenizer from the gevin model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1931520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aff9f50",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a3ba9",
   "metadata": {},
   "source": [
    "Define a preprocessing function that adds a prefix (\"summarize:\") to each input if needed and tokenizes the text for the model. The labels will be the tokenized summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5f586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e898b873",
   "metadata": {},
   "source": [
    "## Step 4: Define the Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc74fc",
   "metadata": {},
   "source": [
    "Use a data collator designed for sequence-to-sequence models, which dynamically pads inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "991d5ded",
   "metadata": {},
   "source": [
    "## Step 5: Load the Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b67c4a",
   "metadata": {},
   "source": [
    "Load the model for sequence-to-sequence tasks (summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af20bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6276996",
   "metadata": {},
   "source": [
    "## Step 6: Define Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef9692",
   "metadata": {},
   "source": [
    "Set up the training configuration with parameters like learning rate, batch size, and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9710ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d109886",
   "metadata": {},
   "source": [
    "## Step 7: Initialize the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26da5a2",
   "metadata": {},
   "source": [
    "Use the `Seq2SeqTrainer` class to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b64d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea0d5d0",
   "metadata": {},
   "source": [
    "## Step 8: Fine-tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b78223",
   "metadata": {},
   "source": [
    "Train the model using the specified arguments and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183e3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc4ae5b",
   "metadata": {},
   "source": [
    "## Step 9: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f30dd0",
   "metadata": {},
   "source": [
    "Once the model is trained, perform inference on a sample text to generate a summary. Use the tokenizer to process the text, and then feed it into the model to get the generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f2fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
