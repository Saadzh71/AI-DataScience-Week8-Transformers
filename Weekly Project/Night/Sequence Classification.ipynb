{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6338e9cf",
   "metadata": {},
   "source": [
    "# Fine-tuning a Sequence Classification Model Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78efcc",
   "metadata": {},
   "source": [
    "In this exam, you will be tasked with performing dataset preprocessing and fine-tuning a model for sequence classification. Complete each step carefully according to the instructions provided.\n",
    "\n",
    "### Model and Dataset Information\n",
    "\n",
    "For this task, you will be working with the following:\n",
    "\n",
    "- **Model Checkpoint**: Use the pre-trained model checkpoint `aubmindlab/bert-base-arabertv02` for both the model and tokenizer.\n",
    "- **Dataset**: You will be using the `CUTD/sanad_df` dataset. Ensure to load and preprocess the dataset correctly for training and evaluation.\n",
    "\n",
    "**Note:**\n",
    "- Any additional steps or methods you include that improve or enhance the results will be rewarded with bonus points if they are justified.\n",
    "- The steps outlined here are suggestions. You are free to implement alternative methods or approaches to achieve the task, as long as you explain the reasoning and the process at the bottom of the notebook.\n",
    "- You can use either TensorFlow or PyTorch for this task. If you prefer TensorFlow, feel free to use it when working with Hugging Face Transformers.\n",
    "- The number of data samples you choose to work with is flexible. However, if you select a very low number of samples and the training time is too short, this could affect the evaluation of your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aded80",
   "metadata": {},
   "source": [
    "## Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54930f9",
   "metadata": {},
   "source": [
    "Load the dataset and split it into training and test sets. Use 20% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64e03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d87ddcd5",
   "metadata": {},
   "source": [
    "## Step 2: Clean Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10d4d4",
   "metadata": {},
   "source": [
    "Remove any columns from the dataset that are not needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd06b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "790b4a9e",
   "metadata": {},
   "source": [
    "## Step 3: Splitting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c8c1f4",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing sets, ensuring that 20% of the data is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f839fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "091d3694",
   "metadata": {},
   "source": [
    "## Step 4: Tokenizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6418768",
   "metadata": {},
   "source": [
    "Initialize a tokenizer for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a28bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89500748",
   "metadata": {},
   "source": [
    "## Step 5: Preprocessing the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd6fab",
   "metadata": {},
   "source": [
    "Map the tokenization function to the dataset. Ensure the text data is processed using truncation to handle sequences that exceed the model's input size. Please do any further preprocessing.\n",
    "\n",
    "**Bonus**: If you performed more comprehensive preprocessing, such as removing links, converting text to lowercase, or applying additional preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd8a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f05ad70",
   "metadata": {},
   "source": [
    "### Step 6: Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87441817",
   "metadata": {},
   "source": [
    "Convert the categorical labels into numerical format using a label encoder if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0ce69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c2cde80",
   "metadata": {},
   "source": [
    "### Step 7: Data Collation for Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130522fd",
   "metadata": {},
   "source": [
    "Prepare the data for training by ensuring all sequences in a batch are padded to the same length. Use a data collator to handle dynamic padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5832f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08125f2b",
   "metadata": {},
   "source": [
    "### Step 8: Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209e8e8",
   "metadata": {},
   "source": [
    "Initialize a sequence classification model using the BERT-based architecture. Set the the right amount of output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45858f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c662a02e",
   "metadata": {},
   "source": [
    "## Step 9: Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc922a37",
   "metadata": {},
   "source": [
    "Define the training arguments, including parameters such as learning rate, batch size, number of epochs, and weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bbafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74aabddb",
   "metadata": {},
   "source": [
    "## Step 10: Trainer Initialization and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b4bf5",
   "metadata": {},
   "source": [
    "Set up the trainer with the model, training arguments, dataset, tokenizer, and data collator. Train the model using the dataset you processed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c24838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7e59cb",
   "metadata": {},
   "source": [
    "## Step 11: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ed70c",
   "metadata": {},
   "source": [
    "Once the model is trained, perform inference on a sample text to evaluate the model's prediction capabilities. Use the tokenizer to process the text, and then feed it into the model to get the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90a5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
